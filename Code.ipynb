{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3514c97d",
   "metadata": {},
   "source": [
    "# YOLOv8 Vegetation Detection Model\n",
    "\n",
    "T·∫°o m√¥ h√¨nh YOLOv8 ƒë·ªÉ detect c·ªè c√¢y v·ªõi 1 class duy nh·∫•t.\n",
    "\n",
    "## Pipeline:\n",
    "1. **Setup Environment** - C√†i ƒë·∫∑t th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "2. **Download Dataset** - T·∫£i dataset c·ªè c√¢y c√¥ng khai  \n",
    "3. **Data Preparation** - Chu·∫©n b·ªã d·ªØ li·ªáu cho YOLO format\n",
    "4. **Model Training** - Train YOLOv8 v·ªõi custom dataset\n",
    "5. **Model Evaluation** - ƒê√°nh gi√° v√† test model\n",
    "6. **Inference** - S·ª≠ d·ª•ng model ƒë·ªÉ detect c·ªè c√¢y\n",
    "\n",
    "## Target: Single class \"vegetation\" detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0899944e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# 1. Setup Environment\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Ki·ªÉm tra GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"üñ•Ô∏è Device: {device}\")\n",
    "\n",
    "# C√†i ƒë·∫∑t ultralytics (YOLOv8)\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    print(\"‚úÖ Ultralytics ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ ƒêang c√†i ƒë·∫∑t ultralytics...\")\n",
    "    os.system(\"pip install ultralytics\")\n",
    "    from ultralytics import YOLO\n",
    "    print(\"‚úÖ Ultralytics ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t th√†nh c√¥ng\")\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c project\n",
    "project_dir = Path(\"./vegetation_detection\")\n",
    "project_dir.mkdir(exist_ok=True)\n",
    "\n",
    "data_dir = project_dir / \"data\"\n",
    "models_dir = project_dir / \"models\"\n",
    "results_dir = project_dir / \"results\"\n",
    "\n",
    "for dir_path in [data_dir, models_dir, results_dir]:\n",
    "    dir_path.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Project structure created:\")\n",
    "print(f\"   Data: {data_dir}\")\n",
    "print(f\"   Models: {models_dir}\")\n",
    "print(f\"   Results: {results_dir}\")\n",
    "\n",
    "# Ki·ªÉm tra phi√™n b·∫£n\n",
    "print(f\"üêç Python: {sys.version}\")\n",
    "print(f\"üî• PyTorch: {torch.__version__}\")\n",
    "print(f\"üì∑ OpenCV: {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab85da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Download Vegetation Dataset\n",
    "import requests\n",
    "import json\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class VegetationDatasetDownloader:\n",
    "    def __init__(self):\n",
    "        self.data_dir = data_dir\n",
    "        self.annotations = []\n",
    "        \n",
    "    def download_oxford_flowers(self):\n",
    "        \"\"\"Download Oxford Flowers 102 dataset (c√≥ s·∫µn vegetation data)\"\"\"\n",
    "        print(\"üì• Downloading Oxford Flowers 102 dataset...\")\n",
    "        \n",
    "        try:\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize((640, 640)),\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "            \n",
    "            # Download train v√† test sets\n",
    "            train_dataset = datasets.Flowers102(\n",
    "                root=str(self.data_dir),\n",
    "                split='train',\n",
    "                download=True,\n",
    "                transform=transform\n",
    "            )\n",
    "            \n",
    "            test_dataset = datasets.Flowers102(\n",
    "                root=str(self.data_dir),\n",
    "                split='test',\n",
    "                download=True,\n",
    "                transform=transform\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Downloaded Flowers102:\")\n",
    "            print(f\"   Train: {len(train_dataset)} images\")\n",
    "            print(f\"   Test: {len(test_dataset)} images\")\n",
    "            print(f\"   Classes: 102 flower species\")\n",
    "            \n",
    "            return train_dataset, test_dataset\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error downloading dataset: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def create_vegetation_samples(self, num_samples=500):\n",
    "        \"\"\"T·∫°o sample dataset t·ª´ internet ho·∫∑c local images\"\"\"\n",
    "        print(\"üåø Creating vegetation sample dataset...\")\n",
    "        \n",
    "        # URLs m·∫´u c·ªßa vegetation images (public domain)\n",
    "        sample_urls = [\n",
    "            \"https://images.unsplash.com/photo-1441974231531-c6227db76b6e?w=640\",\n",
    "            \"https://images.unsplash.com/photo-1574263867128-32d95b2e1b2d?w=640\",\n",
    "            \"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=640\",\n",
    "            \"https://images.unsplash.com/photo-1519389950473-47ba0277781c?w=640\",\n",
    "            \"https://images.unsplash.com/photo-1541781774459-bb2af2f05b55?w=640\"\n",
    "        ]\n",
    "        \n",
    "        images_dir = self.data_dir / \"vegetation_samples\" / \"images\"\n",
    "        labels_dir = self.data_dir / \"vegetation_samples\" / \"labels\"\n",
    "        \n",
    "        images_dir.mkdir(parents=True, exist_ok=True)\n",
    "        labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        downloaded = 0\n",
    "        \n",
    "        for i, url in enumerate(sample_urls):\n",
    "            if downloaded >= num_samples:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                print(f\"   Downloading image {i+1}...\")\n",
    "                response = requests.get(url, stream=True, timeout=10)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    image_path = images_dir / f\"vegetation_{i+1:04d}.jpg\"\n",
    "                    \n",
    "                    with open(image_path, 'wb') as f:\n",
    "                        for chunk in response.iter_content(chunk_size=8192):\n",
    "                            f.write(chunk)\n",
    "                    \n",
    "                    # T·∫°o label file (to√†n b·ªô ·∫£nh l√† vegetation)\n",
    "                    self.create_full_image_label(labels_dir / f\"vegetation_{i+1:04d}.txt\")\n",
    "                    \n",
    "                    downloaded += 1\n",
    "                    print(f\"   ‚úÖ Saved: {image_path.name}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Failed to download {url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"‚úÖ Created {downloaded} vegetation samples\")\n",
    "        return downloaded\n",
    "    \n",
    "    def create_full_image_label(self, label_path):\n",
    "        \"\"\"T·∫°o label file cho to√†n b·ªô ·∫£nh l√† vegetation (class 0)\"\"\"\n",
    "        # YOLO format: class_id center_x center_y width height (normalized)\n",
    "        # To√†n b·ªô ·∫£nh l√† vegetation: class 0, center (0.5, 0.5), size (1.0, 1.0)\n",
    "        with open(label_path, 'w') as f:\n",
    "            f.write(\"0 0.5 0.5 1.0 1.0\\n\")\n",
    "\n",
    "# Download dataset\n",
    "downloader = VegetationDatasetDownloader()\n",
    "\n",
    "# Option 1: Download Oxford Flowers (convert v·ªÅ vegetation class)\n",
    "train_dataset, test_dataset = downloader.download_oxford_flowers()\n",
    "\n",
    "# Option 2: T·∫°o sample dataset\n",
    "if train_dataset is None:\n",
    "    print(\"üì• Creating sample vegetation dataset...\")\n",
    "    sample_count = downloader.create_vegetation_samples(num_samples=100)\n",
    "    \n",
    "print(\"‚úÖ Dataset preparation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db896864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Preparation - Convert to YOLO Format\n",
    "import glob\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class YOLODataPreparer:\n",
    "    def __init__(self, base_dir):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.yolo_dir = self.base_dir / \"yolo_dataset\"\n",
    "        \n",
    "        # T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c YOLO\n",
    "        self.train_images = self.yolo_dir / \"train\" / \"images\"\n",
    "        self.train_labels = self.yolo_dir / \"train\" / \"labels\"\n",
    "        self.val_images = self.yolo_dir / \"val\" / \"images\"\n",
    "        self.val_labels = self.yolo_dir / \"val\" / \"labels\"\n",
    "        \n",
    "        for dir_path in [self.train_images, self.train_labels, self.val_images, self.val_labels]:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def convert_flowers_to_vegetation(self):\n",
    "        \"\"\"Convert Oxford Flowers dataset th√†nh vegetation detection\"\"\"\n",
    "        print(\"üîÑ Converting Oxford Flowers to vegetation dataset...\")\n",
    "        \n",
    "        # T√¨m flowers dataset ƒë√£ download\n",
    "        flowers_dir = self.base_dir / \"flowers-102\"\n",
    "        \n",
    "        if not flowers_dir.exists():\n",
    "            print(\"‚ùå Oxford Flowers dataset not found\")\n",
    "            return False\n",
    "        \n",
    "        # L·∫•y t·∫•t c·∫£ ·∫£nh\n",
    "        image_files = list(flowers_dir.rglob(\"*.jpg\"))\n",
    "        print(f\"   Found {len(image_files)} images\")\n",
    "        \n",
    "        if len(image_files) == 0:\n",
    "            print(\"‚ùå No images found in dataset\")\n",
    "            return False\n",
    "        \n",
    "        # Split train/val (80/20)\n",
    "        train_files, val_files = train_test_split(image_files, test_size=0.2, random_state=42)\n",
    "        \n",
    "        print(f\"   Train: {len(train_files)} images\")\n",
    "        print(f\"   Val: {len(val_files)} images\")\n",
    "        \n",
    "        # Process train set\n",
    "        self.process_image_set(train_files, self.train_images, self.train_labels, \"train\")\n",
    "        \n",
    "        # Process validation set\n",
    "        self.process_image_set(val_files, self.val_images, self.val_labels, \"val\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def process_image_set(self, image_files, images_dir, labels_dir, split_name):\n",
    "        \"\"\"X·ª≠ l√Ω m·ªôt set ·∫£nh (train ho·∫∑c val)\"\"\"\n",
    "        print(f\"   Processing {split_name} set...\")\n",
    "        \n",
    "        for i, image_file in enumerate(image_files):\n",
    "            try:\n",
    "                # Copy ·∫£nh\n",
    "                new_image_name = f\"vegetation_{split_name}_{i:04d}.jpg\"\n",
    "                new_image_path = images_dir / new_image_name\n",
    "                shutil.copy2(image_file, new_image_path)\n",
    "                \n",
    "                # T·∫°o label file (to√†n b·ªô ·∫£nh l√† vegetation)\n",
    "                label_name = new_image_name.replace('.jpg', '.txt')\n",
    "                label_path = labels_dir / label_name\n",
    "                \n",
    "                # YOLO format: class_id center_x center_y width height (normalized)\n",
    "                # Class 0 = vegetation, to√†n b·ªô ·∫£nh\n",
    "                with open(label_path, 'w') as f:\n",
    "                    f.write(\"0 0.5 0.5 1.0 1.0\\n\")\n",
    "                \n",
    "                if (i + 1) % 100 == 0:\n",
    "                    print(f\"     Processed {i + 1}/{len(image_files)} images\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"     ‚ùå Error processing {image_file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"   ‚úÖ {split_name} set completed\")\n",
    "    \n",
    "    def create_yaml_config(self):\n",
    "        \"\"\"T·∫°o file config YAML cho YOLOv8\"\"\"\n",
    "        yaml_content = {\n",
    "            'path': str(self.yolo_dir.absolute()),\n",
    "            'train': 'train/images',\n",
    "            'val': 'val/images',\n",
    "            'nc': 1,  # Number of classes\n",
    "            'names': ['vegetation']  # Class names\n",
    "        }\n",
    "        \n",
    "        yaml_path = self.yolo_dir / \"vegetation_config.yaml\"\n",
    "        \n",
    "        with open(yaml_path, 'w') as f:\n",
    "            yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "        \n",
    "        print(f\"‚úÖ Created YAML config: {yaml_path}\")\n",
    "        return yaml_path\n",
    "    \n",
    "    def verify_dataset(self):\n",
    "        \"\"\"Ki·ªÉm tra dataset sau khi convert\"\"\"\n",
    "        train_images_count = len(list(self.train_images.glob(\"*.jpg\")))\n",
    "        train_labels_count = len(list(self.train_labels.glob(\"*.txt\")))\n",
    "        val_images_count = len(list(self.val_images.glob(\"*.jpg\")))\n",
    "        val_labels_count = len(list(self.val_labels.glob(\"*.txt\")))\n",
    "        \n",
    "        print(f\"üìä Dataset verification:\")\n",
    "        print(f\"   Train: {train_images_count} images, {train_labels_count} labels\")\n",
    "        print(f\"   Val: {val_images_count} images, {val_labels_count} labels\")\n",
    "        \n",
    "        # Ki·ªÉm tra consistency\n",
    "        if train_images_count == train_labels_count and val_images_count == val_labels_count:\n",
    "            print(\"‚úÖ Dataset is consistent\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Dataset has mismatched images and labels\")\n",
    "            return False\n",
    "    \n",
    "    def visualize_sample(self, num_samples=3):\n",
    "        \"\"\"Hi·ªÉn th·ªã m·ªôt s·ªë sample t·ª´ dataset\"\"\"\n",
    "        print(f\"üñºÔ∏è Visualizing {num_samples} random samples...\")\n",
    "        \n",
    "        train_images = list(self.train_images.glob(\"*.jpg\"))\n",
    "        samples = random.sample(train_images, min(num_samples, len(train_images)))\n",
    "        \n",
    "        fig, axes = plt.subplots(1, len(samples), figsize=(15, 5))\n",
    "        if len(samples) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, image_path in enumerate(samples):\n",
    "            # Load v√† hi·ªÉn th·ªã ·∫£nh\n",
    "            image = cv2.imread(str(image_path))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            axes[i].imshow(image)\n",
    "            axes[i].set_title(f\"Vegetation Sample {i+1}\")\n",
    "            axes[i].axis('off')\n",
    "            \n",
    "            # ƒê·ªçc label\n",
    "            label_path = self.train_labels / (image_path.stem + '.txt')\n",
    "            with open(label_path, 'r') as f:\n",
    "                label = f.read().strip()\n",
    "            \n",
    "            print(f\"   Sample {i+1}: {image_path.name} -> Label: {label}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Prepare data\n",
    "data_preparer = YOLODataPreparer(data_dir)\n",
    "\n",
    "# Convert dataset\n",
    "if data_preparer.convert_flowers_to_vegetation():\n",
    "    # T·∫°o YAML config\n",
    "    config_path = data_preparer.create_yaml_config()\n",
    "    \n",
    "    # Verify dataset\n",
    "    data_preparer.verify_dataset()\n",
    "    \n",
    "    # Visualize samples\n",
    "    data_preparer.visualize_sample()\n",
    "    \n",
    "    print(\"‚úÖ Data preparation completed!\")\n",
    "else:\n",
    "    print(\"‚ùå Data preparation failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e404a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Train YOLOv8 Model\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "class VegetationModelTrainer:\n",
    "    def __init__(self, config_path, models_dir):\n",
    "        self.config_path = config_path\n",
    "        self.models_dir = Path(models_dir)\n",
    "        self.model = None\n",
    "        \n",
    "    def load_pretrained_model(self, model_size='n'):\n",
    "        \"\"\"Load pre-trained YOLOv8 model\"\"\"\n",
    "        model_name = f\"yolov8{model_size}.pt\"\n",
    "        \n",
    "        print(f\"üì• Loading YOLOv8{model_size} pre-trained model...\")\n",
    "        \n",
    "        try:\n",
    "            self.model = YOLO(model_name)\n",
    "            print(f\"‚úÖ Loaded {model_name}\")\n",
    "            \n",
    "            # Model info\n",
    "            print(f\"   Model type: {type(self.model.model).__name__}\")\n",
    "            print(f\"   Device: {self.model.device}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading model: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def train_model(self, epochs=50, imgsz=640, batch_size=16):\n",
    "        \"\"\"Train YOLOv8 model v·ªõi vegetation dataset\"\"\"\n",
    "        \n",
    "        if self.model is None:\n",
    "            print(\"‚ùå Model not loaded\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"üöÄ Starting YOLOv8 training...\")\n",
    "        print(f\"   Config: {self.config_path}\")\n",
    "        print(f\"   Epochs: {epochs}\")\n",
    "        print(f\"   Image size: {imgsz}\")\n",
    "        print(f\"   Batch size: {batch_size}\")\n",
    "        print(f\"   Device: {device}\")\n",
    "        \n",
    "        try:\n",
    "            # Training parameters\n",
    "            train_args = {\n",
    "                'data': str(self.config_path),\n",
    "                'epochs': epochs,\n",
    "                'imgsz': imgsz,\n",
    "                'batch': batch_size,\n",
    "                'device': device,\n",
    "                'project': str(self.models_dir),\n",
    "                'name': 'vegetation_detection',\n",
    "                'save': True,\n",
    "                'save_period': 10,  # Save checkpoint every 10 epochs\n",
    "                'patience': 15,     # Early stopping patience\n",
    "                'cache': True,      # Cache images for faster training\n",
    "                'workers': 4,       # Number of worker threads\n",
    "                'optimizer': 'AdamW',\n",
    "                'lr0': 0.01,        # Initial learning rate\n",
    "                'weight_decay': 0.0005,\n",
    "                'warmup_epochs': 3,\n",
    "                'mosaic': 1.0,      # Mosaic augmentation probability\n",
    "                'copy_paste': 0.3   # Copy-paste augmentation probability\n",
    "            }\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Start training\n",
    "            results = self.model.train(**train_args)\n",
    "            \n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"‚úÖ Training completed!\")\n",
    "            print(f\"   Training time: {training_time/60:.2f} minutes\")\n",
    "            print(f\"   Results saved in: {self.models_dir / 'vegetation_detection'}\")\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Training failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def validate_model(self):\n",
    "        \"\"\"Validate trained model\"\"\"\n",
    "        print(\"üîç Validating model...\")\n",
    "        \n",
    "        try:\n",
    "            # Load best model\n",
    "            best_model_path = self.models_dir / \"vegetation_detection\" / \"weights\" / \"best.pt\"\n",
    "            \n",
    "            if best_model_path.exists():\n",
    "                model = YOLO(str(best_model_path))\n",
    "                \n",
    "                # Validate\n",
    "                results = model.val(data=str(self.config_path))\n",
    "                \n",
    "                print(\"‚úÖ Validation completed!\")\n",
    "                print(f\"   mAP50: {results.results_dict.get('metrics/mAP50(B)', 'N/A')}\")\n",
    "                print(f\"   mAP50-95: {results.results_dict.get('metrics/mAP50-95(B)', 'N/A')}\")\n",
    "                \n",
    "                return results\n",
    "            else:\n",
    "                print(f\"‚ùå Best model not found: {best_model_path}\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Validation failed: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = VegetationModelTrainer(\n",
    "    config_path=data_dir / \"yolo_dataset\" / \"vegetation_config.yaml\",\n",
    "    models_dir=models_dir\n",
    ")\n",
    "\n",
    "# Load pre-trained model\n",
    "if trainer.load_pretrained_model(model_size='n'):  # 'n'=nano, 's'=small, 'm'=medium, 'l'=large\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üöÄ STARTING TRAINING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Train model (adjust parameters as needed)\n",
    "    training_results = trainer.train_model(\n",
    "        epochs=30,      # Gi·∫£m epochs cho test nhanh\n",
    "        imgsz=640,      # Image size\n",
    "        batch_size=8    # Gi·∫£m batch size n·∫øu GPU memory nh·ªè\n",
    "    )\n",
    "    \n",
    "    if training_results:\n",
    "        # Validate model\n",
    "        validation_results = trainer.validate_model()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"‚úÖ TRAINING & VALIDATION COMPLETED\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Training failed\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Failed to load pre-trained model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6518bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model Inference v√† Testing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "class VegetationDetector:\n",
    "    def __init__(self, model_path):\n",
    "        \"\"\"Load trained model ƒë·ªÉ inference\"\"\"\n",
    "        self.model_path = Path(model_path)\n",
    "        self.model = None\n",
    "        self.load_model()\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load trained model\"\"\"\n",
    "        if self.model_path.exists():\n",
    "            print(f\"üì• Loading trained model: {self.model_path}\")\n",
    "            try:\n",
    "                self.model = YOLO(str(self.model_path))\n",
    "                print(\"‚úÖ Model loaded successfully!\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading model: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"‚ùå Model file not found: {self.model_path}\")\n",
    "            return False\n",
    "    \n",
    "    def predict_image(self, image_path, conf_threshold=0.5):\n",
    "        \"\"\"Predict vegetation trong ·∫£nh\"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"‚ùå Model not loaded\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            print(f\"üîç Detecting vegetation in: {image_path}\")\n",
    "            \n",
    "            # Run inference\n",
    "            results = self.model(image_path, conf=conf_threshold)\n",
    "            \n",
    "            # Get results\n",
    "            result = results[0]\n",
    "            \n",
    "            print(f\"‚úÖ Detection completed!\")\n",
    "            print(f\"   Detected {len(result.boxes)} vegetation objects\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Prediction error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def visualize_results(self, image_path, result, save_path=None):\n",
    "        \"\"\"Hi·ªÉn th·ªã k·∫øt qu·∫£ detection\"\"\"\n",
    "        \n",
    "        # Load ·∫£nh g·ªëc\n",
    "        image = cv2.imread(str(image_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Create plot\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "        ax.imshow(image)\n",
    "        \n",
    "        # Draw bounding boxes\n",
    "        if result.boxes is not None:\n",
    "            boxes = result.boxes.xyxy.cpu().numpy()  # x1, y1, x2, y2\n",
    "            confidences = result.boxes.conf.cpu().numpy()\n",
    "            \n",
    "            for i, (box, conf) in enumerate(zip(boxes, confidences)):\n",
    "                x1, y1, x2, y2 = box\n",
    "                width = x2 - x1\n",
    "                height = y2 - y1\n",
    "                \n",
    "                # Draw rectangle\n",
    "                rect = patches.Rectangle(\n",
    "                    (x1, y1), width, height,\n",
    "                    linewidth=2, edgecolor='lime', facecolor='none'\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                # Add label\n",
    "                label = f'Vegetation {conf:.2f}'\n",
    "                ax.text(x1, y1-5, label, \n",
    "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lime', alpha=0.7),\n",
    "                       fontsize=10, color='black')\n",
    "        \n",
    "        ax.set_title(f'Vegetation Detection Results\\nDetected: {len(result.boxes)} objects')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "            print(f\"üíæ Results saved: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def test_on_validation_set(self, val_images_dir, num_samples=5):\n",
    "        \"\"\"Test model tr√™n validation set\"\"\"\n",
    "        \n",
    "        val_images = list(Path(val_images_dir).glob(\"*.jpg\"))\n",
    "        test_samples = random.sample(val_images, min(num_samples, len(val_images)))\n",
    "        \n",
    "        print(f\"üß™ Testing on {len(test_samples)} validation samples...\")\n",
    "        \n",
    "        results_summary = []\n",
    "        \n",
    "        for i, image_path in enumerate(test_samples):\n",
    "            print(f\"\\nTesting sample {i+1}/{len(test_samples)}: {image_path.name}\")\n",
    "            \n",
    "            # Predict\n",
    "            result = self.predict_image(image_path)\n",
    "            \n",
    "            if result:\n",
    "                num_detections = len(result.boxes) if result.boxes is not None else 0\n",
    "                \n",
    "                if num_detections > 0:\n",
    "                    avg_confidence = result.boxes.conf.mean().item()\n",
    "                else:\n",
    "                    avg_confidence = 0.0\n",
    "                \n",
    "                results_summary.append({\n",
    "                    'image': image_path.name,\n",
    "                    'detections': num_detections,\n",
    "                    'avg_confidence': avg_confidence\n",
    "                })\n",
    "                \n",
    "                # Visualize\n",
    "                save_path = results_dir / f\"test_result_{i+1}.jpg\"\n",
    "                self.visualize_results(image_path, result, save_path)\n",
    "        \n",
    "        # Summary\n",
    "        print(f\"\\nüìä Test Results Summary:\")\n",
    "        print(f\"   Total samples: {len(results_summary)}\")\n",
    "        \n",
    "        if results_summary:\n",
    "            total_detections = sum(r['detections'] for r in results_summary)\n",
    "            avg_detections = total_detections / len(results_summary)\n",
    "            avg_confidence = np.mean([r['avg_confidence'] for r in results_summary if r['avg_confidence'] > 0])\n",
    "            \n",
    "            print(f\"   Total detections: {total_detections}\")\n",
    "            print(f\"   Average detections per image: {avg_detections:.2f}\")\n",
    "            print(f\"   Average confidence: {avg_confidence:.3f}\")\n",
    "        \n",
    "        return results_summary\n",
    "\n",
    "# Test trained model\n",
    "best_model_path = models_dir / \"vegetation_detection\" / \"weights\" / \"best.pt\"\n",
    "\n",
    "if best_model_path.exists():\n",
    "    print(\"üéØ Testing trained vegetation detection model...\")\n",
    "    \n",
    "    # Initialize detector\n",
    "    detector = VegetationDetector(best_model_path)\n",
    "    \n",
    "    if detector.model:\n",
    "        # Test tr√™n validation set\n",
    "        val_images_dir = data_dir / \"yolo_dataset\" / \"val\" / \"images\"\n",
    "        \n",
    "        if val_images_dir.exists():\n",
    "            test_results = detector.test_on_validation_set(val_images_dir, num_samples=3)\n",
    "            \n",
    "            print(\"‚úÖ Model testing completed!\")\n",
    "        else:\n",
    "            print(\"‚ùå Validation images directory not found\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Trained model not found. Run training first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49daa44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Save v√† Deploy Model\n",
    "import onnx\n",
    "import shutil\n",
    "\n",
    "class VegetationModelExporter:\n",
    "    def __init__(self, model_path, export_dir):\n",
    "        self.model_path = Path(model_path)\n",
    "        self.export_dir = Path(export_dir)\n",
    "        self.export_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        if self.model_path.exists():\n",
    "            self.model = YOLO(str(model_path))\n",
    "        else:\n",
    "            self.model = None\n",
    "            print(f\"‚ùå Model not found: {model_path}\")\n",
    "    \n",
    "    def export_to_onnx(self):\n",
    "        \"\"\"Export model sang ONNX format\"\"\"\n",
    "        if not self.model:\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            print(\"üì§ Exporting model to ONNX format...\")\n",
    "            \n",
    "            onnx_path = self.export_dir / \"vegetation_detector.onnx\"\n",
    "            \n",
    "            # Export to ONNX\n",
    "            self.model.export(\n",
    "                format='onnx',\n",
    "                imgsz=640,\n",
    "                opset=11,\n",
    "                simplify=True,\n",
    "                dynamic=False\n",
    "            )\n",
    "            \n",
    "            # Move exported file\n",
    "            exported_onnx = self.model_path.parent / (self.model_path.stem + \".onnx\")\n",
    "            if exported_onnx.exists():\n",
    "                shutil.move(str(exported_onnx), str(onnx_path))\n",
    "                print(f\"‚úÖ ONNX model saved: {onnx_path}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"‚ùå ONNX export failed\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ONNX export error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def export_to_torchscript(self):\n",
    "        \"\"\"Export model sang TorchScript format\"\"\"\n",
    "        if not self.model:\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            print(\"üì§ Exporting model to TorchScript format...\")\n",
    "            \n",
    "            torchscript_path = self.export_dir / \"vegetation_detector.torchscript\"\n",
    "            \n",
    "            # Export to TorchScript\n",
    "            self.model.export(\n",
    "                format='torchscript',\n",
    "                imgsz=640\n",
    "            )\n",
    "            \n",
    "            # Move exported file\n",
    "            exported_ts = self.model_path.parent / (self.model_path.stem + \".torchscript\")\n",
    "            if exported_ts.exists():\n",
    "                shutil.move(str(exported_ts), str(torchscript_path))\n",
    "                print(f\"‚úÖ TorchScript model saved: {torchscript_path}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"‚ùå TorchScript export failed\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå TorchScript export error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def create_deployment_package(self):\n",
    "        \"\"\"T·∫°o package ho√†n ch·ªânh cho deployment\"\"\"\n",
    "        print(\"üì¶ Creating deployment package...\")\n",
    "        \n",
    "        package_dir = self.export_dir / \"deployment_package\"\n",
    "        package_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Copy model files\n",
    "        model_files = list(self.export_dir.glob(\"vegetation_detector.*\"))\n",
    "        \n",
    "        for model_file in model_files:\n",
    "            shutil.copy2(model_file, package_dir / model_file.name)\n",
    "            print(f\"   ‚úÖ Copied: {model_file.name}\")\n",
    "        \n",
    "        # Copy original PyTorch model\n",
    "        if self.model_path.exists():\n",
    "            shutil.copy2(self.model_path, package_dir / \"vegetation_detector.pt\")\n",
    "            print(f\"   ‚úÖ Copied: vegetation_detector.pt\")\n",
    "        \n",
    "        # Create inference script\n",
    "        self.create_inference_script(package_dir)\n",
    "        \n",
    "        # Create requirements.txt\n",
    "        self.create_requirements_file(package_dir)\n",
    "        \n",
    "        # Create README\n",
    "        self.create_readme_file(package_dir)\n",
    "        \n",
    "        print(f\"‚úÖ Deployment package created: {package_dir}\")\n",
    "        return package_dir\n",
    "    \n",
    "    def create_inference_script(self, package_dir):\n",
    "        \"\"\"T·∫°o script inference ƒë∆°n gi·∫£n\"\"\"\n",
    "        script_content = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Vegetation Detection Inference Script\n",
    "Usage: python inference.py --image path/to/image.jpg\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class VegetationDetector:\n",
    "    def __init__(self, model_path=\"vegetation_detector.pt\"):\n",
    "        self.model = YOLO(model_path)\n",
    "    \n",
    "    def detect(self, image_path, conf_threshold=0.5):\n",
    "        \"\"\"Detect vegetation in image\"\"\"\n",
    "        results = self.model(image_path, conf=conf_threshold)\n",
    "        return results[0]\n",
    "    \n",
    "    def save_results(self, image_path, result, output_path):\n",
    "        \"\"\"Save detection results\"\"\"\n",
    "        # Load image\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        # Draw detections\n",
    "        if result.boxes is not None:\n",
    "            boxes = result.boxes.xyxy.cpu().numpy()\n",
    "            confidences = result.boxes.conf.cpu().numpy()\n",
    "            \n",
    "            for box, conf in zip(boxes, confidences):\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                \n",
    "                # Draw rectangle\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                \n",
    "                # Add label\n",
    "                label = f'Vegetation {conf:.2f}'\n",
    "                cv2.putText(image, label, (x1, y1-10), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Save result\n",
    "        cv2.imwrite(output_path, image)\n",
    "        print(f\"Results saved: {output_path}\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Vegetation Detection')\n",
    "    parser.add_argument('--image', required=True, help='Input image path')\n",
    "    parser.add_argument('--output', default='result.jpg', help='Output image path')\n",
    "    parser.add_argument('--conf', type=float, default=0.5, help='Confidence threshold')\n",
    "    parser.add_argument('--model', default='vegetation_detector.pt', help='Model path')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Initialize detector\n",
    "    detector = VegetationDetector(args.model)\n",
    "    \n",
    "    # Run detection\n",
    "    result = detector.detect(args.image, args.conf)\n",
    "    \n",
    "    # Save results\n",
    "    detector.save_results(args.image, result, args.output)\n",
    "    \n",
    "    # Print summary\n",
    "    num_detections = len(result.boxes) if result.boxes is not None else 0\n",
    "    print(f\"Detected {num_detections} vegetation objects\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "        \n",
    "        script_path = package_dir / \"inference.py\"\n",
    "        with open(script_path, 'w') as f:\n",
    "            f.write(script_content)\n",
    "        \n",
    "        print(f\"   ‚úÖ Created: inference.py\")\n",
    "    \n",
    "    def create_requirements_file(self, package_dir):\n",
    "        \"\"\"T·∫°o requirements.txt\"\"\"\n",
    "        requirements = [\n",
    "            \"ultralytics>=8.0.0\",\n",
    "            \"opencv-python>=4.5.0\",\n",
    "            \"numpy>=1.21.0\",\n",
    "            \"torch>=1.11.0\",\n",
    "            \"torchvision>=0.12.0\",\n",
    "            \"Pillow>=8.3.0\",\n",
    "            \"matplotlib>=3.5.0\"\n",
    "        ]\n",
    "        \n",
    "        req_path = package_dir / \"requirements.txt\"\n",
    "        with open(req_path, 'w') as f:\n",
    "            f.write('\\n'.join(requirements))\n",
    "        \n",
    "        print(f\"   ‚úÖ Created: requirements.txt\")\n",
    "    \n",
    "    def create_readme_file(self, package_dir):\n",
    "        \"\"\"T·∫°o README.md\"\"\"\n",
    "        readme_content = '''# Vegetation Detection Model\n",
    "\n",
    "YOLOv8-based vegetation detection model trained for single-class vegetation detection.\n",
    "\n",
    "## Files\n",
    "- `vegetation_detector.pt` - PyTorch model (recommended)\n",
    "- `vegetation_detector.onnx` - ONNX model (cross-platform)\n",
    "- `vegetation_detector.torchscript` - TorchScript model\n",
    "- `inference.py` - Inference script\n",
    "- `requirements.txt` - Python dependencies\n",
    "\n",
    "## Installation\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "## Usage\n",
    "```bash\n",
    "# Basic inference\n",
    "python inference.py --image input.jpg --output result.jpg\n",
    "\n",
    "# With custom confidence threshold\n",
    "python inference.py --image input.jpg --output result.jpg --conf 0.7\n",
    "\n",
    "# Using ONNX model\n",
    "python inference.py --image input.jpg --model vegetation_detector.onnx\n",
    "```\n",
    "\n",
    "## Python API\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load model\n",
    "model = YOLO('vegetation_detector.pt')\n",
    "\n",
    "# Run inference\n",
    "results = model('image.jpg')\n",
    "\n",
    "# Process results\n",
    "for result in results:\n",
    "    boxes = result.boxes.xyxy  # Bounding boxes\n",
    "    confs = result.boxes.conf  # Confidences\n",
    "    print(f\"Detected {len(boxes)} vegetation objects\")\n",
    "```\n",
    "\n",
    "## Model Details\n",
    "- **Task**: Object Detection\n",
    "- **Classes**: 1 (vegetation)\n",
    "- **Input Size**: 640x640\n",
    "- **Framework**: YOLOv8\n",
    "- **Format**: PyTorch (.pt), ONNX (.onnx), TorchScript (.torchscript)\n",
    "\n",
    "## Performance\n",
    "- Optimized for vegetation detection in natural scenes\n",
    "- Single class detection reduces false positives\n",
    "- Real-time inference capability\n",
    "'''\n",
    "        \n",
    "        readme_path = package_dir / \"README.md\"\n",
    "        with open(readme_path, 'w') as f:\n",
    "            f.write(readme_content)\n",
    "        \n",
    "        print(f\"   ‚úÖ Created: README.md\")\n",
    "\n",
    "# Export v√† deploy model\n",
    "if best_model_path.exists():\n",
    "    print(\"üöÄ Exporting trained model...\")\n",
    "    \n",
    "    exporter = VegetationModelExporter(\n",
    "        model_path=best_model_path,\n",
    "        export_dir=results_dir / \"exported_models\"\n",
    "    )\n",
    "    \n",
    "    # Export to different formats\n",
    "    onnx_success = exporter.export_to_onnx()\n",
    "    torchscript_success = exporter.export_to_torchscript()\n",
    "    \n",
    "    # Create deployment package\n",
    "    if onnx_success or torchscript_success:\n",
    "        package_dir = exporter.create_deployment_package()\n",
    "        \n",
    "        print(\"\\n‚úÖ Model export completed!\")\n",
    "        print(f\"üì¶ Deployment package: {package_dir}\")\n",
    "        print(\"\\nüìã Available formats:\")\n",
    "        print(\"   - PyTorch (.pt) - Recommended for Python\")\n",
    "        if onnx_success:\n",
    "            print(\"   - ONNX (.onnx) - Cross-platform deployment\")\n",
    "        if torchscript_success:\n",
    "            print(\"   - TorchScript (.torchscript) - Production deployment\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No trained model found. Complete training first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f0dca",
   "metadata": {},
   "source": [
    "# üéâ Ho√†n th√†nh! H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng\n",
    "\n",
    "## üìã T√≥m t·∫Øt qu√° tr√¨nh:\n",
    "\n",
    "1. **‚úÖ Setup Environment** - C√†i ƒë·∫∑t YOLOv8 v√† dependencies\n",
    "2. **‚úÖ Download Dataset** - Oxford Flowers 102 dataset (100+ classes hoa)\n",
    "3. **‚úÖ Data Preparation** - Convert th√†nh 1 class \"vegetation\" trong YOLO format\n",
    "4. **‚úÖ Model Training** - Train YOLOv8 v·ªõi custom vegetation dataset\n",
    "5. **‚úÖ Model Testing** - Test tr√™n validation set\n",
    "6. **‚úÖ Model Export** - Export sang nhi·ªÅu format (PT, ONNX, TorchScript)\n",
    "\n",
    "## üöÄ C√°ch ch·∫°y t·ª´ng b∆∞·ªõc:\n",
    "\n",
    "### B∆∞·ªõc 1: Ch·∫°y Setup\n",
    "```python\n",
    "# Ch·∫°y cell 1 ƒë·ªÉ setup environment\n",
    "```\n",
    "\n",
    "### B∆∞·ªõc 2: Download Data  \n",
    "```python\n",
    "# Ch·∫°y cell 2 ƒë·ªÉ download Oxford Flowers dataset\n",
    "# Dataset s·∫Ω ƒë∆∞·ª£c convert th√†nh single class \"vegetation\"\n",
    "```\n",
    "\n",
    "### B∆∞·ªõc 3: Prepare Data\n",
    "```python  \n",
    "# Ch·∫°y cell 3 ƒë·ªÉ convert sang YOLO format\n",
    "# T·∫°o train/val split v√† YAML config\n",
    "```\n",
    "\n",
    "### B∆∞·ªõc 4: Train Model\n",
    "```python\n",
    "# Ch·∫°y cell 4 ƒë·ªÉ train YOLOv8\n",
    "# C√≥ th·ªÉ adjust epochs, batch_size, model_size\n",
    "```\n",
    "\n",
    "### B∆∞·ªõc 5: Test Model\n",
    "```python\n",
    "# Ch·∫°y cell 5 ƒë·ªÉ test model tr√™n validation set\n",
    "# Visualize detection results\n",
    "```\n",
    "\n",
    "### B∆∞·ªõc 6: Export Model\n",
    "```python\n",
    "# Ch·∫°y cell 6 ƒë·ªÉ export model sang c√°c format kh√°c nhau\n",
    "# T·∫°o deployment package\n",
    "```\n",
    "\n",
    "## üéØ K·∫øt qu·∫£ mong ƒë·ª£i:\n",
    "\n",
    "- **Model**: `vegetation_detector.pt` detect class \"vegetation\" \n",
    "- **Performance**: mAP50 > 0.8 (depends on dataset quality)\n",
    "- **Speed**: Real-time inference tr√™n GPU\n",
    "- **Deployment**: Ready-to-use package v·ªõi inference script\n",
    "\n",
    "## üîß Customization:\n",
    "\n",
    "### Thay ƒë·ªïi dataset:\n",
    "```python\n",
    "# Trong cell 2, thay ƒë·ªïi downloader ƒë·ªÉ d√πng dataset kh√°c\n",
    "# V√≠ d·ª•: Agriculture-Vision, DeepWeeds, v.v.\n",
    "```\n",
    "\n",
    "### Thay ƒë·ªïi model size:\n",
    "```python  \n",
    "# Trong cell 4, thay ƒë·ªïi model_size\n",
    "model_size = 'n'  # nano (fastest)\n",
    "model_size = 's'  # small  \n",
    "model_size = 'm'  # medium\n",
    "model_size = 'l'  # large (best accuracy)\n",
    "```\n",
    "\n",
    "### Thay ƒë·ªïi training parameters:\n",
    "```python\n",
    "# Trong cell 4, adjust training args\n",
    "epochs = 100        # More epochs = better accuracy  \n",
    "batch_size = 16     # Larger batch = faster training\n",
    "imgsz = 640         # Larger image = better accuracy\n",
    "```\n",
    "\n",
    "## üì± S·ª≠ d·ª•ng model:\n",
    "\n",
    "### Inference ƒë∆°n gi·∫£n:\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('vegetation_detector.pt')\n",
    "results = model('image.jpg')\n",
    "\n",
    "# Xem k·∫øt qu·∫£\n",
    "for result in results:\n",
    "    print(f\"Detected {len(result.boxes)} vegetation objects\")\n",
    "```\n",
    "\n",
    "### Inference v·ªõi custom threshold:\n",
    "```python\n",
    "results = model('image.jpg', conf=0.7)  # 70% confidence\n",
    "```\n",
    "\n",
    "### Batch inference:\n",
    "```python\n",
    "results = model(['img1.jpg', 'img2.jpg', 'img3.jpg'])\n",
    "```\n",
    "\n",
    "## üåü Tips ƒë·ªÉ c·∫£i thi·ªán model:\n",
    "\n",
    "1. **More Data**: Th√™m nhi·ªÅu ·∫£nh vegetation ƒëa d·∫°ng\n",
    "2. **Data Augmentation**: TƒÉng augmentation trong training\n",
    "3. **Longer Training**: Train nhi·ªÅu epochs h∆°n  \n",
    "4. **Larger Model**: D√πng YOLOv8m ho·∫∑c YOLOv8l\n",
    "5. **Better Annotations**: Label ch√≠nh x√°c h∆°n (kh√¥ng ph·∫£i to√†n b·ªô ·∫£nh)\n",
    "\n",
    "## üîç Troubleshooting:\n",
    "\n",
    "- **Out of Memory**: Gi·∫£m batch_size v√† imgsz\n",
    "- **Low mAP**: TƒÉng epochs, check data quality\n",
    "- **Slow Training**: S·ª≠ d·ª•ng GPU, gi·∫£m image size\n",
    "- **No Detections**: Gi·∫£m confidence threshold\n",
    "\n",
    "Ready to detect vegetation! üåøüöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envChipAI38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
